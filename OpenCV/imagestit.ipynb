{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032ffed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f22bcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Loaded Image 1: C:\\Users\\daksh\\MachineLearning\\OpenCV\\WhatsApp Image 2025-11-18 at 11.12.46 PM.jpeg\n",
      "\n",
      "[INFO] Loaded Image 2: C:\\Users\\daksh\\MachineLearning\\OpenCV\\WhatsApp Image 2025-11-18 at 11.12.47 PM (1).jpeg\n",
      "\n",
      "[INFO] Loaded Image 3: C:\\Users\\daksh\\MachineLearning\\OpenCV\\WhatsApp Image 2025-11-18 at 11.12.48 PM.jpeg\n"
     ]
    }
   ],
   "source": [
    "# ==== 1. Load Images ====\n",
    "\n",
    "num_imgs = int(input(\"How many images do you want to stitch? (min 2): \"))\n",
    "\n",
    "paths = []\n",
    "for i in range(num_imgs):\n",
    "    p = input(f\"Enter path for image {i+1}: \")\n",
    "    paths.append(p)\n",
    "\n",
    "images = []\n",
    "for i, p in enumerate(paths):\n",
    "    img = cv2.imread(p)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"[ERROR] Could not load: {p}\")\n",
    "        continue\n",
    "\n",
    "    images.append(img)\n",
    "    print(f\"\\n[INFO] Loaded Image {i+1}: {p}\")\n",
    "\n",
    "    # Show image correctly (VS Code friendly)\n",
    "    cv2.imshow(f\"Image {i+1}\", img)\n",
    "    cv2.waitKey(1000)        # show for 1 second (you can change this)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if len(images) < 2:\n",
    "    raise ValueError(\"Need at least 2 valid images to stitch!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b7f08b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Converted Image 1 to grayscale\n",
      "[INFO] Converted Image 2 to grayscale\n",
      "[INFO] Converted Image 3 to grayscale\n",
      "\n",
      "[INFO] Showing grayscale versions:\n",
      "Gray Image 1\n",
      "Gray Image 2\n",
      "Gray Image 3\n"
     ]
    }
   ],
   "source": [
    "gray_images = []\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray_images.append(gray)\n",
    "    print(f\"[INFO] Converted Image {i+1} to grayscale\")\n",
    "\n",
    "# Show grayscale images (VS Code compatible)\n",
    "print(\"\\n[INFO] Showing grayscale versions:\")\n",
    "for i, g in enumerate(gray_images):\n",
    "    print(f\"Gray Image {i+1}\")\n",
    "\n",
    "    cv2.imshow(f\"Grayscale Image {i+1}\", g)\n",
    "    cv2.waitKey(1000)   # show for 1 second (you can change it)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6299fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Image 1: 2003 keypoints detected\n",
      "[INFO] Image 2: 2812 keypoints detected\n",
      "[INFO] Image 3: 4000 keypoints detected\n"
     ]
    }
   ],
   "source": [
    "orb = cv2.ORB_create(nfeatures=4000)\n",
    "\n",
    "kp_list = []\n",
    "des_list = []\n",
    "\n",
    "for i, gray in enumerate(gray_images):\n",
    "\n",
    "    # Detect keypoints + descriptors\n",
    "    keypts, des = orb.detectAndCompute(gray, None)\n",
    "    print(f\"[INFO] Image {i+1}: {len(keypts)} keypoints detected\")\n",
    "\n",
    "    kp_list.append(keypts)\n",
    "    des_list.append(des)\n",
    "\n",
    "    # Visualize keypoints\n",
    "    kp_img = cv2.drawKeypoints(\n",
    "        gray, keypts, None,\n",
    "        color=(0, 255, 0),\n",
    "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "    )\n",
    "\n",
    "    # Show in VS Code\n",
    "    cv2.imshow(f\"Keypoints Image {i+1}\", kp_img)\n",
    "    cv2.waitKey(1000)     # show for 1 second (change to 0 for manual close)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3b6a23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Matching Image 1 with Image 2 ===\n",
      "[INFO] Good Matches: 293\n",
      "\n",
      "=== Matching Image 2 with Image 3 ===\n",
      "[INFO] Good Matches: 151\n"
     ]
    }
   ],
   "source": [
    "def match_features_orb(desc1, desc2):\n",
    "    # safety checks\n",
    "    if desc1 is None or desc2 is None:\n",
    "        print(\"[ERROR] Descriptor missing — cannot match\")\n",
    "        return []\n",
    "\n",
    "    if len(desc1) == 0 or len(desc2) == 0:\n",
    "        print(\"[ERROR] Empty descriptor set — cannot match\")\n",
    "        return []\n",
    "\n",
    "    # ORB uses Hamming distance\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "\n",
    "    # KNN matches\n",
    "    matches = bf.knnMatch(desc1, desc2, k=2)\n",
    "\n",
    "    # Lowe’s ratio test\n",
    "    good = [m for m, n in matches if m.distance < 0.75 * n.distance]\n",
    "\n",
    "    print(f\"[INFO] Good Matches: {len(good)}\")\n",
    "    return good\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# AUTOMATIC MATCHING FOR N IMAGES (VS Code)\n",
    "# ============================================================\n",
    "\n",
    "all_matches = []\n",
    "\n",
    "for i in range(len(des_list) - 1):\n",
    "    print(f\"\\n=== Matching Image {i+1} with Image {i+2} ===\")\n",
    "\n",
    "    desc1 = des_list[i]\n",
    "    desc2 = des_list[i + 1]\n",
    "\n",
    "    good = match_features_orb(desc1, desc2)\n",
    "\n",
    "    all_matches.append(good)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2662215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_matches(img1, kp1, img2, kp2, matches, max_show=50):\n",
    "    if len(matches) == 0:\n",
    "        print(\"[WARN] No matches to visualize\")\n",
    "        return\n",
    "\n",
    "    matched_img = cv2.drawMatches(\n",
    "        img1, kp1,\n",
    "        img2, kp2,\n",
    "        matches[:max_show], None,\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )\n",
    "\n",
    "    cv2.imshow(\"Matches\", matched_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a837a082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Showing matches for Image 1 & Image 2\n",
      "\n",
      "Showing matches for Image 2 & Image 3\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_matches)):\n",
    "    print(f\"\\nShowing matches for Image {i+1} & Image {i+2}\")\n",
    "    draw_matches(images[i], kp_list[i], images[i+1], kp_list[i+1], all_matches[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0635d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 4. FEATHER BLENDING FUNCTION (must come BEFORE warp_and_merge)\n",
    "# ==========================================================\n",
    "\n",
    "def feather_blend(base, overlay):\n",
    "    base_gray = cv2.cvtColor(base, cv2.COLOR_BGR2GRAY)\n",
    "    over_gray = cv2.cvtColor(overlay, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    base_mask = (base_gray > 0).astype(np.uint8)\n",
    "    over_mask = (over_gray > 0).astype(np.uint8)\n",
    "\n",
    "    if base_mask.max() == 0:\n",
    "        return overlay\n",
    "    if over_mask.max() == 0:\n",
    "        return base\n",
    "\n",
    "    dist_base = cv2.distanceTransform(base_mask, cv2.DIST_L2, 3)\n",
    "    dist_over = cv2.distanceTransform(over_mask, cv2.DIST_L2, 3)\n",
    "\n",
    "    weight_base = dist_base / (dist_base + dist_over + 1e-8)\n",
    "    weight_over = dist_over / (dist_base + dist_over + 1e-8)\n",
    "\n",
    "    weight_base = weight_base[..., None]\n",
    "    weight_over = weight_over[..., None]\n",
    "\n",
    "    blended = base * weight_base + overlay * weight_over\n",
    "    return blended.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1170ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# AUTO-CROP BLACK BORDERS (must come BEFORE stitching loop)\n",
    "# ==========================================================\n",
    "\n",
    "def auto_crop_black(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # create binary mask of non-black region\n",
    "    _, thresh = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    coords = cv2.findNonZero(thresh)\n",
    "    if coords is None:\n",
    "        return image\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(coords)\n",
    "    return image[y:y+h, x:x+w]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ebcfa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_and_merge(img1, img2, H):\n",
    "    # Ensure H is float64\n",
    "    H = H.astype(np.float64)\n",
    "\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "\n",
    "    # Corners of image 2\n",
    "    corners_img2 = np.float32([\n",
    "        [0, 0],\n",
    "        [0, h2],\n",
    "        [w2, h2],\n",
    "        [w2, 0]\n",
    "    ]).reshape(-1, 1, 2)\n",
    "\n",
    "    warped_corners_img2 = cv2.perspectiveTransform(corners_img2, H)\n",
    "\n",
    "    # Corners of image 1\n",
    "    corners_img1 = np.float32([\n",
    "        [0, 0],\n",
    "        [0, h1],\n",
    "        [w1, h1],\n",
    "        [w1, 0]\n",
    "    ]).reshape(-1, 1, 2)\n",
    "\n",
    "    # All corners combined\n",
    "    all_corners = np.concatenate((corners_img1, warped_corners_img2), axis=0)\n",
    "\n",
    "    # Bounding box for panorama\n",
    "    xmin, ymin = np.int32(all_corners.min(axis=0).ravel() - 0.5)\n",
    "    xmax, ymax = np.int32(all_corners.max(axis=0).ravel() + 0.5)\n",
    "\n",
    "    # Translate so image fits whole canvas\n",
    "    tx, ty = -xmin, -ymin\n",
    "    T = np.array([[1, 0, tx],\n",
    "                  [0, 1, ty],\n",
    "                  [0, 0, 1]], dtype=np.float64)\n",
    "\n",
    "    size = (xmax - xmin, ymax - ymin)\n",
    "\n",
    "    # Warp image 2 into the panorama frame\n",
    "    warped_img2 = cv2.warpPerspective(img2, T @ H, size)\n",
    "\n",
    "    # Place image 1 onto canvas\n",
    "    canvas1 = np.zeros_like(warped_img2)\n",
    "    canvas1[ty:ty + h1, tx:tx + w1] = img1\n",
    "\n",
    "    # Blend both images\n",
    "    blended = feather_blend(canvas1, warped_img2)\n",
    "\n",
    "    return blended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abebcf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP] Stitching image 2\n",
      "[INFO] Good Matches: 293\n",
      "[INFO] Homography:\n",
      "[[ 6.26998098e-01 -1.26874194e-02  3.45941364e+02]\n",
      " [-2.71100531e-01  7.91645587e-01  1.68171374e+02]\n",
      " [-3.75853289e-04 -1.04408919e-04  1.00000000e+00]]\n",
      "\n",
      "[STEP] Stitching image 3\n",
      "[INFO] Good Matches: 94\n",
      "[INFO] Homography:\n",
      "[[ 1.17888635e-01 -2.14789113e-02  9.21775654e+02]\n",
      " [-7.58742697e-01  8.87670546e-01  1.90383566e+02]\n",
      " [-9.51441653e-04 -1.50943207e-04  1.00000000e+00]]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 102873895137 bytes in function 'cv::OutOfMemoryError'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(H)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Warp + blend\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m panorama \u001b[38;5;241m=\u001b[39m warp_and_merge(panorama, images[i], H)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Crop black borders\u001b[39;00m\n\u001b[0;32m     37\u001b[0m panorama \u001b[38;5;241m=\u001b[39m auto_crop_black(panorama)\n",
      "Cell \u001b[1;32mIn[26], line 42\u001b[0m, in \u001b[0;36mwarp_and_merge\u001b[1;34m(img1, img2, H)\u001b[0m\n\u001b[0;32m     39\u001b[0m size \u001b[38;5;241m=\u001b[39m (xmax \u001b[38;5;241m-\u001b[39m xmin, ymax \u001b[38;5;241m-\u001b[39m ymin)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Warp image 2 into the panorama frame\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m warped_img2 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpPerspective(img2, T \u001b[38;5;241m@\u001b[39m H, size)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Place image 1 onto canvas\u001b[39;00m\n\u001b[0;32m     45\u001b[0m canvas1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(warped_img2)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 102873895137 bytes in function 'cv::OutOfMemoryError'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 7. STITCHING LOOP (AUTOMATIC FOR N IMAGES)\n",
    "# ==========================================================\n",
    "\n",
    "panorama = images[0]   # start with first image\n",
    "\n",
    "for i in range(1, len(images)):\n",
    "    print(f\"\\n[STEP] Stitching image {i+1}\")\n",
    "\n",
    "    # ORB features for the current panorama\n",
    "    gray_pano = cv2.cvtColor(panorama, cv2.COLOR_BGR2GRAY)\n",
    "    kp_pano, des_pano = orb.detectAndCompute(gray_pano, None)\n",
    "\n",
    "    # Precomputed features of next image\n",
    "    kp_next = kp_list[i]\n",
    "    des_next = des_list[i]\n",
    "\n",
    "    # Feature match\n",
    "    matches = match_features_orb(des_pano, des_next)\n",
    "\n",
    "    if len(matches) < 4:\n",
    "        print(\"[ERROR] Not enough matches — skipping image\", i+1)\n",
    "        continue\n",
    "\n",
    "    # Homography\n",
    "    src_pts = np.float32([kp_pano[m.queryIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([kp_next[m.trainIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "\n",
    "    H, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "    print(\"[INFO] Homography:\")\n",
    "    print(H)\n",
    "\n",
    "    # Warp + blend\n",
    "    panorama = warp_and_merge(panorama, images[i], H)\n",
    "\n",
    "    # Crop black borders\n",
    "    panorama = auto_crop_black(panorama)\n",
    "\n",
    "    # Show progress\n",
    "    cv2.imshow(\"Panorama Progress\", panorama)\n",
    "    cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fbf56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
